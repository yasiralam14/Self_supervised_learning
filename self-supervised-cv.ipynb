{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:57:22.335537Z","iopub.execute_input":"2024-12-09T22:57:22.335796Z","iopub.status.idle":"2024-12-09T22:57:23.408312Z","shell.execute_reply.started":"2024-12-09T22:57:22.335771Z","shell.execute_reply":"2024-12-09T22:57:23.407671Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torchvision.transforms import ToTensor\nfrom torchvision import transforms\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms, models\nfrom datasets import load_dataset\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader  \nimport matplotlib.pyplot as plt\nfrom datasets import load_dataset\nimport torch.distributed as dist\nfrom torchvision.models import resnet18\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:57:26.240164Z","iopub.execute_input":"2024-12-09T22:57:26.240924Z","iopub.status.idle":"2024-12-09T22:57:33.317685Z","shell.execute_reply.started":"2024-12-09T22:57:26.240893Z","shell.execute_reply":"2024-12-09T22:57:33.316793Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"#Image net Tiny\nds = load_dataset(\"zh-plus/tiny-imagenet\")\nprint(ds)\npretrain = ds['train']\nfinetune = ds['valid']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:58:45.223949Z","iopub.execute_input":"2024-12-09T22:58:45.224288Z","iopub.status.idle":"2024-12-09T22:58:46.322619Z","shell.execute_reply.started":"2024-12-09T22:58:45.224259Z","shell.execute_reply":"2024-12-09T22:58:46.321853Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 100000\n    })\n    valid: Dataset({\n        features: ['image', 'label'],\n        num_rows: 10000\n    })\n})\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"**Barlow twin pretraining**","metadata":{}},{"cell_type":"code","source":"reduce_size =     transforms.Resize((64))\n\nrandom_transform = transforms.Compose([\n    transforms.RandomResizedCrop(64),\n    transforms.RandomHorizontalFlip(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\nclass Encoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = resnet18(pretrained=False)\n        self.backbone.fc = nn.Identity()  # Remove classification head\n        self.projection = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Linear(256, 256)\n        )\n    \n    def forward(self, x):\n        x = self.backbone(x)\n        x = self.projection(x)\n        return x\n\n# Loss function for cross-correlation\ndef barlow_twins_loss(z1, z2, lambda_=5e-3):\n    # Batch size\n    batch_size = z1.size(0)\n    \n    # Normalize the embeddings\n    z1_norm = (z1 - z1.mean(0)) / z1.std(0)\n    z2_norm = (z2 - z2.mean(0)) / z2.std(0)\n    \n    # Compute cross-correlation matrix\n    c = torch.mm(z1_norm.T, z2_norm) / batch_size\n    \n    # Identity matrix\n    on_diag = torch.diagonal(c).add_(-1).pow(2).sum()  # On-diagonal terms\n    off_diag = (c - torch.eye(c.size(0), device=c.device)).pow(2).sum() - on_diag  # Off-diagonal terms\n    \n    # Loss\n    return on_diag + lambda_ * off_diag\n\n\nclass PreTrainDataset(Dataset):\n    def __init__(self, data ,transform=None):\n        self.data = data\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img = self.data[idx]['image']\n        if not img.mode == \"RGB\":\n            img = img.convert(\"RGB\")\n        img=ToTensor()(img)\n        if self.transform:\n            img = self.transform(img)\n        return img\n\nclass TestDataset(Dataset):\n    def __init__(self, data ,transform=None):\n        self.data = data\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img = self.data[idx]['image']\n        label = self.data[idx]['label']\n        if not img.mode == \"RGB\":\n            img = img.convert(\"RGB\")\n        img=ToTensor()(img)\n        if self.transform:\n            img = self.transform(img)\n        return img,label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:39:53.139859Z","iopub.execute_input":"2024-12-09T23:39:53.140734Z","iopub.status.idle":"2024-12-09T23:39:53.153475Z","shell.execute_reply.started":"2024-12-09T23:39:53.140698Z","shell.execute_reply":"2024-12-09T23:39:53.152237Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"**Fine Tuning**","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndataset = PreTrainDataset(pretrain,transform=reduce_size)\nloader = DataLoader(dataset, batch_size=1024, shuffle=True)\n\nencoder = Encoder().to(device)\nencoder = nn.DataParallel(encoder)\noptimizer = optim.Adam(encoder.parameters(), lr=0.001)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:03:20.444830Z","iopub.execute_input":"2024-12-09T23:03:20.445421Z","iopub.status.idle":"2024-12-09T23:03:20.650632Z","shell.execute_reply.started":"2024-12-09T23:03:20.445388Z","shell.execute_reply":"2024-12-09T23:03:20.649914Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Training loop for barlow twins\nfor epoch in range(10):  # Number of epochs\n    encoder.train()\n    for x in loader:\n        # Move data to GPU\n        x = x.to(device)\n        \n        # Apply two random augmentations\n        x_a = random_transform(x)\n        x_b = random_transform(x)\n        \n        # Forward passes\n        z_a = encoder(x_a)\n        z_b = encoder(x_b)\n        \n        # Compute loss\n        loss = barlow_twins_loss(z_a, z_b)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:13:26.614322Z","iopub.execute_input":"2024-12-09T23:13:26.615128Z","iopub.status.idle":"2024-12-09T23:22:49.388242Z","shell.execute_reply.started":"2024-12-09T23:13:26.615093Z","shell.execute_reply":"2024-12-09T23:22:49.387336Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 19.6493\nEpoch 2, Loss: 11.5978\nEpoch 3, Loss: 9.1249\nEpoch 4, Loss: 8.1447\nEpoch 5, Loss: 9.5854\nEpoch 6, Loss: 9.0360\nEpoch 7, Loss: 25.7458\nEpoch 8, Loss: 7.0035\nEpoch 9, Loss: 10.0565\nEpoch 10, Loss: 7.7338\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"class DownstreamModel(nn.Module):\n    def __init__(self, encoder, num_classes):\n        super().__init__()\n        if encoder.module:\n            self.encoder = encoder.module\n        else:\n            self.encoder = encoder\n        self.head = nn.Linear(512, num_classes)  # Output features = num_classes\n\n    def forward(self, x):\n        x = self.encoder.backbone(x)  # Use only the backbone\n        x = self.head(x)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:55:50.478847Z","iopub.execute_input":"2024-12-09T23:55:50.479696Z","iopub.status.idle":"2024-12-09T23:55:50.484638Z","shell.execute_reply.started":"2024-12-09T23:55:50.479644Z","shell.execute_reply":"2024-12-09T23:55:50.483713Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"test_data = TestDataset(finetune,transform=reduce_size)\ntest_loader = loader = DataLoader(test_data, batch_size=1024, shuffle=False)\nmodel = DownstreamModel(encoder,200).to(device)\nmodel = nn.DataParallel(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:55:57.143979Z","iopub.execute_input":"2024-12-09T23:55:57.144310Z","iopub.status.idle":"2024-12-09T23:55:57.152247Z","shell.execute_reply.started":"2024-12-09T23:55:57.144282Z","shell.execute_reply":"2024-12-09T23:55:57.151410Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"\n# Set the model in evaluation mode\nmodel.eval()\n\n# List to store predictions and ground truth labels\nall_preds = []\nall_labels = []\n\n# Disable gradient computation for inference\nwith torch.no_grad():\n    for inputs, labels in test_loader:  # Assuming test_loader provides batches\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        # Forward pass through the model\n        outputs = model(inputs)  # Get model predictions\n        \n        # Get the predicted class (for classification, typically the class with the max probability)\n        _, preds = torch.max(outputs, 1)  # '1' indicates class-wise max\n        \n        # Store predictions and true labels\n        all_preds.append(preds.cpu().numpy())  # Move predictions to CPU and convert to numpy\n        all_labels.append(labels.cpu().numpy())  # Move true labels to CPU and convert to numpy\n\n# Convert lists to numpy arrays\nall_preds = np.concatenate(all_preds)\nall_labels = np.concatenate(all_labels)\n\n# Compute accuracy\naccuracy = np.sum(all_preds == all_labels) / len(all_labels)\nprint(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:55:59.512354Z","iopub.execute_input":"2024-12-09T23:55:59.513143Z","iopub.status.idle":"2024-12-09T23:56:07.402819Z","shell.execute_reply.started":"2024-12-09T23:55:59.513107Z","shell.execute_reply":"2024-12-09T23:56:07.401957Z"}},"outputs":[{"name":"stdout","text":"Test Accuracy: 0.41%\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# Helper functions\nclass PILDataset(Dataset):\n    def __init__(self, data ,transform=None):\n        self.data = data\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img,label = self.data[idx]['image'],self.data[idx]['label']\n        if not img.mode == \"RGB\":\n            img = img.convert(\"RGB\")\n        img=ToTensor()(img)\n        label = self.labels[idx]\n        if self.transform:\n            img = self.transform(img)\n        return img,label\ntrain_transforms = transforms.Compose([\n    # Resize the image to a fixed size (if needed)\n    transforms.Resize((64, 64)),\n    \n    # Randomly crop the image to add variety\n    transforms.RandomCrop(64, padding=4),\n    \n    # Randomly flip the image horizontally with a 50% chance\n    transforms.RandomHorizontalFlip(),\n    \n    # Apply a slight random blur to the image\n    transforms.GaussianBlur(kernel_size=(3, 3)),\n    \n    # Convert the image tensor to grayscale (if you want grayscale images)\n    # Uncomment if needed:\n    # transforms.Grayscale(num_output_channels=3),\n    \n    # Normalize the image tensor (after applying augmentation)\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n    \n    # Optionally, add a color jitter for brightness, contrast, saturation, and hue\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n])\n\neval_transforms = transforms.Compose([\n    # Resize the image to a fixed size (if needed)\n    transforms.Resize((64, 64)),\n    \n    # Randomly crop the image to add variety\n    transforms.RandomCrop(64, padding=4),\n    \n    # Randomly flip the image horizontally with a 50% chance\n    transforms.RandomHorizontalFlip(),\n    \n    # Apply a slight random blur to the image\n    transforms.GaussianBlur(kernel_size=(3, 3)),\n    \n    # Convert the image tensor to grayscale (if you want grayscale images)\n    # Uncomment if needed:\n    # transforms.Grayscale(num_output_channels=3),\n    \n    # Normalize the image tensor (after applying augmentation)\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n    \n    # Optionally, add a color jitter for brightness, contrast, saturation, and hue\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n])\n\n\n\n\ndef show_image(image,size):\n    plt.figure(figsize=(size, size))\n    plt.imshow(image)\n    plt.axis('off')  # Hide axes for better display\n    plt.show()\n\ndef data_split(dataset,test_size=0.1,labels=None,seed =42):\n# DATASET IS PIL DATASET\n    indices  =range(len(dataset))\n    pre_train_indices, train_indices = train_test_split(\n        indices, \n        test_size=test_size, \n        random_state=seed, \n        stratify=labels ) # Use labels for stratification\n    train= dataset.select(pre_train_indices)\n    test = dataset.select(train_indices)\n\n    return train,test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:57:47.907325Z","iopub.execute_input":"2024-12-09T22:57:47.907974Z","iopub.status.idle":"2024-12-09T22:57:47.926594Z","shell.execute_reply.started":"2024-12-09T22:57:47.907930Z","shell.execute_reply":"2024-12-09T22:57:47.925793Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# # whole imagenet\n# ds = load_dataset(\"evanarlian/imagenet_1k_resized_256\")\n# print(ds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T22:59:00.198971Z","iopub.execute_input":"2024-12-09T22:59:00.199295Z","iopub.status.idle":"2024-12-09T22:59:00.203113Z","shell.execute_reply.started":"2024-12-09T22:59:00.199266Z","shell.execute_reply":"2024-12-09T22:59:00.202211Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train_dataset = PILDataset(train)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n\nfor images, labels in train_loader:\n    print(images.shape, labels.shape)\n    break  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:07:49.872609Z","iopub.execute_input":"2024-12-09T20:07:49.873269Z","iopub.status.idle":"2024-12-09T20:07:49.989773Z","shell.execute_reply.started":"2024-12-09T20:07:49.873240Z","shell.execute_reply":"2024-12-09T20:07:49.988842Z"}},"outputs":[{"name":"stdout","text":"torch.Size([64, 3, 64, 64]) torch.Size([64])\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"\nmodel = models.resnet18(weights=None)\nmodel.fc = nn.Linear(model.fc.in_features, 200)  # Tiny ImageNet has 200 classes\n\nmodel.train()\n\ncriterion = nn.CrossEntropyLoss() \noptimizer = optim.Adam(model.parameters(), lr=0.001)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:12:06.496127Z","iopub.execute_input":"2024-12-09T20:12:06.496795Z","iopub.status.idle":"2024-12-09T20:12:06.726057Z","shell.execute_reply.started":"2024-12-09T20:12:06.496761Z","shell.execute_reply":"2024-12-09T20:12:06.725113Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"num_epochs = 10\nmodel.to(device)\nmodel = nn.DataParallel(model)\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    for images,labels in train_loader:\n        # Get inputs and labels\n        images, labels = images.to(device), labels.to(device)\n\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(images)\n       \n\n        # Calculate loss\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n\n        # Update the loss and accuracy\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n# 8. Training loop\n\n\n    # Print the loss and accuracy for every epoch\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100 * correct/total:.2f}%\")\n\nprint(\"Finished Training\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T20:12:10.040269Z","iopub.execute_input":"2024-12-09T20:12:10.040844Z","iopub.status.idle":"2024-12-09T20:12:21.367051Z","shell.execute_reply.started":"2024-12-09T20:12:10.040812Z","shell.execute_reply":"2024-12-09T20:12:21.365865Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images,labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Get inputs and labels\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Zero the parameter gradients\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[3], line 15\u001b[0m, in \u001b[0;36mPILDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m img\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     14\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m img\u001b[38;5;241m=\u001b[39m\u001b[43mToTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], F_pil\u001b[38;5;241m.\u001b[39mget_image_num_channels(pic))\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T23:00:06.563476Z","iopub.execute_input":"2024-12-09T23:00:06.564258Z","iopub.status.idle":"2024-12-09T23:00:06.614522Z","shell.execute_reply.started":"2024-12-09T23:00:06.564222Z","shell.execute_reply":"2024-12-09T23:00:06.613520Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}